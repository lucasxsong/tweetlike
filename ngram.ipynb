{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "import json\n",
    "from collections import Counter\n",
    "import random\n",
    "import re\n",
    " \n",
    "# \"...\" -> {...}\n",
    "def extract_ngrams(data, num):\n",
    "    n_grams = ngrams(data.split(' '), num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "# remove urls\n",
    "def clean_text(txt):\n",
    "    # remove urls\n",
    "    txt = re.sub(r\"[A-Za-z]+://[A-Za-z]+.[A-Za-z]*[/.].*\", '',txt)\n",
    "    # remove unescaped characters\n",
    "    txt = re.sub(r\"&[A-Za-z]+;\", \"\", txt)\n",
    "#     txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return txt \n",
    "\n",
    "# generates a sentence seed\n",
    "def startsent(bigrams):\n",
    "    beg = []\n",
    "    for b in bigrams:\n",
    "        first = b.split(' ')[0]\n",
    "        # TO DO: CHANGE CHOOSE CONDITION DYNAMICALLY BASED ON FILE SIZE\n",
    "        if (first == \"begsent\" and bigrams[b] > 1):\n",
    "            beg.append(b)\n",
    "            \n",
    "    return random.choice(beg)\n",
    "\n",
    "# gets the next word given the previous\n",
    "# random choice from 10 most popular ngrams\n",
    "def getword(word, bigrams):\n",
    "    possiblewords = []\n",
    "    newword=\"\"\n",
    "    \n",
    "    for b in bigrams:\n",
    "        first = b.split(' ')[0]\n",
    "        \n",
    "        # don't want to add too irrelevant bigrams to choose from\n",
    "        # TO DO: CHANGE CHOOSE CONDITION DYNAMICALLY BASED ON FILE SIZE\n",
    "        if (first == word and bigrams[b] > 0):\n",
    "            newword = b.split(' ')[1]\n",
    "            possiblewords.append(newword)\n",
    "            \n",
    "    print(len(possiblewords))\n",
    "\n",
    "    if (len(possiblewords) == 0):\n",
    "        return \"\"\n",
    "    else:\n",
    "        newword = random.choice(possiblewords)\n",
    "        if(newword == \"endsent\"): return \"\"\n",
    "        else: return newword\n",
    "    \n",
    "\n",
    "def word_gen(bigrams):\n",
    "    seed = startsent(bigrams)\n",
    "    sent = seed.split(' ')[1]\n",
    "    \n",
    "    nextword = getword(sent, bigrams)\n",
    "    while (nextword != \"\" and len(sent) < 140):\n",
    "        sent += \" \" + nextword\n",
    "        nextword = getword(nextword, bigrams)\n",
    "        \n",
    "    # if too many @'s, retry\n",
    "    if(len(re.findall('@', sent)) == len(sent.split(' '))):\n",
    "       sent = word_gen(bigrams)\n",
    "    \n",
    "    return sent\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9237\n",
      "3\n",
      "122\n",
      "9\n",
      "4\n",
      "1\n",
      "122\n",
      "1\n",
      "87\n",
      "1\n",
      "5\n",
      "2\n",
      "146\n",
      "1\n",
      "4\n",
      "15\n",
      "1\n",
      "@dianarene21 and then being pregnant and repost? i added heart issues to develop it. it’s driv…\n"
     ]
    }
   ],
   "source": [
    "ifile = open(\"ngramsbycity/idaho\")\n",
    "bgrams = json.load(ifile)\n",
    "\n",
    "# print(bgrams)\n",
    "print(len(bgrams))\n",
    "print(word_gen(bgrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "### takin an n grams approach ###\n",
    "def writengrams(city):\n",
    "    with open(\"cities/\" + city, \"r\") as content:\n",
    "        data = json.load(content)\n",
    "\n",
    "    all_tweets = []\n",
    "    for tweet in data:\n",
    "        updatetweet = \"begsent \" + tweet['tweet'][0]['text'] + \" endsent\"\n",
    "        all_tweets.append(updatetweet)\n",
    "\n",
    "    all_tweets = [h for h in all_tweets if h != \"Unknown\"]\n",
    "\n",
    "    corpus = [clean_text(x) for x in all_tweets]\n",
    "\n",
    "    bigrams = {}\n",
    "\n",
    "    for tweet in corpus:\n",
    "        bigram = extract_ngrams(tweet.lower(), 2)\n",
    "        for b in bigram:\n",
    "            if b in bigrams:\n",
    "                bigrams[b] +=1\n",
    "            else:\n",
    "                bigrams[b] = 1\n",
    "            \n",
    "    sortedbigrams = {k: v for k, v in sorted(bigrams.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    with open(\"ngramsbycity/\" + city, 'w') as file:\n",
    "        file.write(json.dumps(sortedbigrams))\n",
    "    \n",
    "    \n",
    "    \n",
    "# print(Counter(allngs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE TWEETS -> NGRAM FILES ###\n",
    "\n",
    "cityfile = open('writtencities', \"r\")\n",
    "cities = json.load(cityfile)\n",
    "\n",
    "for f in cities:\n",
    "    city = re.sub(\" \", \"\", f.split(',')[0].lower())\n",
    "    writengrams(city)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
